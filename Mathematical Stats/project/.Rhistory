pvalTable <- cbind(colnames(disease)[1:62], result)
View(pvalTable)
View(pvalTable)
# Problem 3, Glaucoma
disease <- read.csv(file="glaucoma.csv", sep = ",", header = TRUE)
attach(disease)
result = c()
for(i in 1:62){
x <- disease[,i][Class == "normal"]
y <- disease[,i][Class == "glaucoma" ]
result[i] <- t.test(x,y,alternative="two.sided")$p.value
}
pvalTable <- cbind(colnames(disease)[1:62], result)
names <- colnames(disease)
eyebox <- function(n){
m <- n + 14
par(mfrow=c(3,5))
for(i in n:m){
boxplot(disease[,i]~Class, main=names[i])
}
}
eyebox(1)
eyebox(16)
eyebox(46)
# make the fit
fit <- rpart(Class~., data=disease, method="class")
# make the fit
library(rpart)
fit <- rpart(Class~., data=disease, method="class")
fit
prediction <- predict(fit)
head(prediction)
gCorr <- sum(prediction[,1]<=)0.5 & Class == 'glaucoma')
gCorr <- sum(prediction[,1]<=)0.5 & Class == 'glaucoma'))
gCorr <- sum(prediction[,1]<=0.5 & Class == 'glaucoma')
gCorr  <- sum(prediction[,1] <= 0.5 & Class == 'glaucoma')
gIncor <- sum(prediction[,1] > 0.5  & Class == 'glaucoma')
nCorr  <- sum(prediction[,2] <= 0.5 & Class == 'normal')
nIncor <- sum(prediction[,2] > 0.5  & Class == 'normal')
table(prediction)
prediction <- predict(fit, type="class")
head(prediction)
table(prediction)
prediction <- predict(fit)
head(prediction)
gCorr  <- sum(prediction[,1] <= 0.5 & Class == 'glaucoma')
gIncor <- sum(prediction[,1] > 0.5  & Class == 'glaucoma')
nCorr  <- sum(prediction[,2] <= 0.5 & Class == 'normal')
nIncor <- sum(prediction[,2] > 0.5  & Class == 'normal')
fish <- read.csv("fish.csv",sep = ",",header = TRUE)
# pair test
t.test(fish$Guarding, fish$Fanning, paired = TRUE)
# sign test
guarding <- fish$Guarding
fanning <- fish$Fanning
diff <- fish$Guarding - fish$Fanning
# choosing an alpha of 0.05
z_alpha = 1.96
n = length(diff)
k = sum(diff > 0)
z = (k - n/2)/(sqrt(n/4))
binom.test(k,n)
fish <- read.csv("fish.csv",sep = ",",header = TRUE)
# pair test
t.test(fish$Guarding, fish$Fanning, paired = TRUE)
# sign test
guarding <- fish$Guarding
fanning <- fish$Fanning
diff <- fish$Guarding - fish$Fanning
# choosing an alpha of 0.05
z_alpha = 1.96
n = length(diff)
k = sum(diff > 0)
z = (k - n/2)/(sqrt(n/4))
# split into groups now
fish_split <- split(fish, fish$pH)
fish_ph55 <- fish_split$`5.5`
fish_ph6 <- fish_split$`6`
fish_ph65 <- fish_split$`6.5`
fish_ph7 <- fish_split$`7`
# tests for pH 5.5
t.test(fish_ph55$Guarding, fish_ph55$Fanning, paired = TRUE) # t = 9.8986, alt hyp
binom.test(sum((fish_ph55$Guarding - fish_ph55$Fanning) > 0), length(fish_ph55$Guarding)) #p-val = 4.005e-5
# tests for ph6
t.test(fish_ph6$Guarding, fish_ph6$Fanning, paired = TRUE) # t = 4.1506, alt hyp
binom.test(sum((fish_ph6$Guarding - fish_ph6$Fanning) > 0), length(fish_ph6$Guarding)) #p-val = 0.04139
# test for ph6.5
t.test(fish_ph65$Guarding, fish_ph65$Fanning, paired = TRUE) # t = 5.3816, alt hyp
binom.test(sum((fish_ph65$Guarding - fish_ph65$Fanning) > 0), length(fish_ph65$Guarding)) #p-val = .0004025
# test for ph7
t.test(fish_ph7$Guarding, fish_ph7$Fanning, paired = TRUE) # t = 5.3816, alt hyp
binom.test(sum((fish_ph7$Guarding - fish_ph7$Fanning) > 0), length(fish_ph7$Guarding)) #p-val = .8238
# Split by genders
fish_gender_split <- split(fish, fish$Gender)
fish_male <- fish_gender_split$M
fish_female <- fish_gender_split$F
# male test
t.test(fish_male$Guarding, fish_male$Fanning, paired = TRUE) # p-val = 2.2e-16
binom.test(sum((fish_male$Guarding - fish_male$Fanning)>0), length(fish_male$Guarding)) #p-val = 1.8e-12
# female test
t.test(fish_female$Guarding, fish_female$Fanning, paired=TRUE) #p-val = 0.1455
binom.test(sum((fish_female$Guarding - fish_female$Fanning)>0), length(fish_female$Guarding)) #p-val = 0.4296
# now it is time to start plotting things
library(ggplot2)
# Guarding vs Fanning colored by Gender
p <- ggplot(fish, aes(x=Guarding, y=Fanning, color=Gender)) + geom_point()
p + ggtitle("Fish Guarding times vs Fanning times")
# pH plots
par(mfrow=c(2,2))
plot(fish_ph55$Guarding, fish_ph55$Fanning, xlab = 'Guarding', ylab = 'Fanning', main = 'pH 5.5')
plot(fish_ph6$Guarding, fish_ph6$Fanning, xlab = 'Guarding', ylab = 'Fanning', main = 'pH 6.0')
plot(fish_ph65$Guarding, fish_ph65$Fanning, xlab = 'Guarding', ylab = 'Fanning', main = 'pH 6.5')
plot(fish_ph7$Guarding, fish_ph7$Fanning, xlab = 'Guarding', ylab = 'Fanning', main = 'pH 7.0')
# tests for pH 5.5
t.test(fish_ph55$Guarding, fish_ph55$Fanning, paired = TRUE) # t = 9.8986, alt hyp
binom.test(sum((fish_ph55$Guarding - fish_ph55$Fanning) > 0), length(fish_ph55$Guarding)) #p-val = 4.005e-5
# tests for ph6
t.test(fish_ph6$Guarding, fish_ph6$Fanning, paired = TRUE) # t = 4.1506, alt hyp
binom.test(sum((fish_ph6$Guarding - fish_ph6$Fanning) > 0), length(fish_ph6$Guarding)) #p-val = 0.04139
# test for ph7
t.test(fish_ph7$Guarding, fish_ph7$Fanning, paired = TRUE) # t = 5.3816, alt hyp
binom.test(sum((fish_ph7$Guarding - fish_ph7$Fanning) > 0), length(fish_ph7$Guarding)) #p-val = .8238
# test for ph7
t.test(fish_ph7$Guarding, fish_ph7$Fanning, paired = TRUE) # t = 5.3816, alt hyp
# tests for pH 5.5
t.test(fish_ph55$Guarding, fish_ph55$Fanning, paired = TRUE) # t = 9.8986, alt hyp
binom.test(sum((fish_ph55$Guarding - fish_ph55$Fanning) > 0), length(fish_ph55$Guarding)) #p-val = 4.005e-5
# tests for ph6
t.test(fish_ph6$Guarding, fish_ph6$Fanning, paired = TRUE) # t = 4.1506, alt hyp
binom.test(sum((fish_ph6$Guarding - fish_ph6$Fanning) > 0), length(fish_ph6$Guarding)) #p-val = 0.04139
# test for ph6.5
t.test(fish_ph65$Guarding, fish_ph65$Fanning, paired = TRUE) # t = 5.3816, alt hyp
binom.test(sum((fish_ph65$Guarding - fish_ph65$Fanning) > 0), length(fish_ph65$Guarding)) #p-val = .0004025
# test for ph7
t.test(fish_ph7$Guarding, fish_ph7$Fanning, paired = TRUE) # t = 5.3816, alt hyp
# male test
t.test(fish_male$Guarding, fish_male$Fanning, paired = TRUE) # p-val = 2.2e-16
# male test
t.test(fish_male$Guarding, fish_male$Fanning, paired = TRUE) # p-val = 2.2e-16
binom.test(sum((fish_male$Guarding - fish_male$Fanning)>0), length(fish_male$Guarding)) #p-val = 1.8e-12
# female test
t.test(fish_female$Guarding, fish_female$Fanning, paired=TRUE) #p-val = 0.1455
# Problem 3, Glaucoma
disease <- read.csv(file="glaucoma.csv", sep = ",", header = TRUE)
attach(disease)
result = c()
for(i in 1:62){
x <- disease[,i][Class == "normal"]
y <- disease[,i][Class == "glaucoma" ]
result[i] <- t.test(x,y,alternative="two.sided")$p.value
}
pvalTable <- cbind(colnames(disease)[1:62], result)
names <- colnames(disease)
eyebox <- function(n){
m <- n + 14
par(mfrow=c(3,5))
for(i in n:m){
boxplot(disease[,i]~Class, main=names[i])
}
}
eyebox(1)
eyebox(16)
eyebox(46)
# make the fit
library(rpart)
fit <- rpart(Class~., data=disease, method="class")
fit
prediction <- predict(fit)
head(prediction)
gCorr  <- sum(prediction[,1] <= 0.5 & Class == 'glaucoma')
gIncor <- sum(prediction[,1] > 0.5  & Class == 'glaucoma')
nCorr  <- sum(prediction[,2] <= 0.5 & Class == 'normal')
nIncor <- sum(prediction[,2] > 0.5  & Class == 'normal')
gCorr  <- sum(prediction[,1] <= 0.5 & Class == 'glaucoma')
gIncor <- sum(prediction[,1] > 0.5  & Class == 'glaucoma')
nCorr  <- sum(prediction[,2] <= 0.5 & Class == 'normal')
nIncor <- sum(prediction[,2] > 0.5  & Class == 'normal')
head(prediction)
# Problem 2, Generic Problem set with Regression
generic <- read.csv("generic.csv", sep = ",", header = TRUE)
# getting tired of typing generic$(blah), substituting
y <- generic$y
x1 <- generic$x1
x2 <- generic$x2
x3 <- generic$x3
x4 <- generic$x4
x5 <- generic$x5
x6 <- generic$x6
x7 <- generic$x7
# exploratory data analysis
par(mfrow=c(2,4), mai=c(0.3,0.1,0.2,0.1))
hist(generic$y, main = "Response Histogram") # right-skewed
hist(generic$x1, main = "x1 histogram") # about normal
hist(generic$x2, main = "x2 histogram") # about normal
hist(generic$x3, main = "x3 histogram") # 'uniform' but not
hist(generic$x4, main = "x4 histogram") # about normal
hist(generic$x5, main = "x5 histogram") # 'uniform'
hist(generic$x6, main = "x6 histogram") # almost bimodal?
hist(generic$x7, main = "x7 histogram") # straight up 0 or 1, maybe categorical
# Check some correlations
cor(x1, y) # very low, 0.03185
cor(x2, y) # negative, -0.4132
cor(x3, y) # positive, 0.41152
cor(x4, y) # small negative, -0.00868
cor(x5, y) # positive, big 0.5486
cor(x6, y) # low positive, 0.02811
cor(x7, y) # positive, 0.31876
# do some models and grab summaries from it
model1 = lm(y~x1)
summary(model1) # r^2 horrible
model2 = lm(y~x2)
summary(model2) # r^2 = .1623, ehh
model3 = lm(y~x3)
summary(model3) # r^2 = .1609
model4 = lm(y~x4)
summary(model4) # r2 = -0.01013, bad
model5 = lm(y~x5)
summary(model5) # r2 = 0.2938
model6 = lm(y~x6)
summary(model6) # r2 = -0.009405
model7 = lm(y~x7)
summary(model7) # r2 = 0.09244
# combined model
combinedModel <- lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(combinedModel)
# the t-values that are most significant are x2 (-1.528), x3 (5.863),
# x5 (6.819), x7 (5.811)
# plot the residuals vs each predictor variable
plot(x1, res)
plot(x2, res)
plot(x3, res)
plot(x4, res)
plot(x5, res)
plot(x6, res)
plot(x7, res)
# do some sort of transformation and check the model summary on that
testModel1 <- lm(y~ x3 + x5^2 + x7)
summary(testModel1)
# Problem 2, Generic Problem set with Regression
generic <- read.csv("generic.csv", sep = ",", header = TRUE)
# getting tired of typing generic$(blah), substituting
y <- generic$y
x1 <- generic$x1
x2 <- generic$x2
x3 <- generic$x3
x4 <- generic$x4
x5 <- generic$x5
x6 <- generic$x6
x7 <- generic$x7
# exploratory data analysis
par(mfrow=c(2,4), mai=c(0.3,0.1,0.2,0.1))
hist(generic$y, main = "Response Histogram") # right-skewed
hist(generic$x1, main = "x1 histogram") # about normal
hist(generic$x2, main = "x2 histogram") # about normal
hist(generic$x3, main = "x3 histogram") # 'uniform' but not
hist(generic$x4, main = "x4 histogram") # about normal
hist(generic$x5, main = "x5 histogram") # 'uniform'
hist(generic$x6, main = "x6 histogram") # almost bimodal?
hist(generic$x7, main = "x7 histogram") # straight up 0 or 1, maybe categorical
# Check some correlations
cor(x1, y) # very low, 0.03185
cor(x2, y) # negative, -0.4132
cor(x3, y) # positive, 0.41152
cor(x4, y) # small negative, -0.00868
cor(x5, y) # positive, big 0.5486
cor(x6, y) # low positive, 0.02811
cor(x7, y) # positive, 0.31876
# do some models and grab summaries from it
model1 = lm(y~x1)
summary(model1) # r^2 horrible
model2 = lm(y~x2)
summary(model2) # r^2 = .1623, ehh
model3 = lm(y~x3)
summary(model3) # r^2 = .1609
model4 = lm(y~x4)
summary(model4) # r2 = -0.01013, bad
model5 = lm(y~x5)
summary(model5) # r2 = 0.2938
model6 = lm(y~x6)
summary(model6) # r2 = -0.009405
model7 = lm(y~x7)
summary(model7) # r2 = 0.09244
# combined model
combinedModel <- lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(combinedModel)
# the t-values that are most significant are x2 (-1.528), x3 (5.863),
# x5 (6.819), x7 (5.811)
res <- resid(combinedModel)
# plot the residuals vs each predictor variable
plot(x1, res)
plot(x2, res)
plot(x3, res)
plot(x4, res)
plot(x5, res)
plot(x6, res)
plot(x7, res)
# do some sort of transformation and check the model summary on that
testModel1 <- lm(y~ x3 + x5^2 + x7)
summary(testModel1)
hist(res)
# do some sort of transformation and check the model summary on that
y <- y^2
testModel1 <- lm(y~ x3 + x5^2 + x7)
summary(testModel1)
# do some sort of transformation and check the model summary on that
y <- sqrt(y)
testModel1 <- lm(y~ x3 + x5^2 + x7)
summary(testModel1)
# do some sort of transformation and check the model summary on that
y <- log(y)
testModel1 <- lm(y~ x3 + x5^2 + x7)
summary(testModel1)
# do some sort of transformation and check the model summary on that
y <- log(y)
testModel1 <- lm(y~ x3^2 + x5 + x7)
summary(testModel1)
# Problem 2, Generic Problem set with Regression
generic <- read.csv("generic.csv", sep = ",", header = TRUE)
# getting tired of typing generic$(blah), substituting
y <- generic$y
x1 <- generic$x1
x2 <- generic$x2
x3 <- generic$x3
x4 <- generic$x4
x5 <- generic$x5
x6 <- generic$x6
x7 <- generic$x7
# exploratory data analysis
par(mfrow=c(2,4), mai=c(0.3,0.1,0.2,0.1))
hist(generic$y, main = "Response Histogram") # right-skewed
hist(generic$x1, main = "x1 histogram") # about normal
hist(generic$x2, main = "x2 histogram") # about normal
hist(generic$x3, main = "x3 histogram") # 'uniform' but not
hist(generic$x4, main = "x4 histogram") # about normal
hist(generic$x5, main = "x5 histogram") # 'uniform'
hist(generic$x6, main = "x6 histogram") # almost bimodal?
hist(generic$x7, main = "x7 histogram") # straight up 0 or 1, maybe categorical
# Check some correlations
cor(x1, y) # very low, 0.03185
cor(x2, y) # negative, -0.4132
cor(x3, y) # positive, 0.41152
cor(x4, y) # small negative, -0.00868
cor(x5, y) # positive, big 0.5486
cor(x6, y) # low positive, 0.02811
cor(x7, y) # positive, 0.31876
# do some models and grab summaries from it
model1 = lm(y~x1)
summary(model1) # r^2 horrible
model2 = lm(y~x2)
summary(model2) # r^2 = .1623, ehh
model3 = lm(y~x3)
summary(model3) # r^2 = .1609
model4 = lm(y~x4)
summary(model4) # r2 = -0.01013, bad
model5 = lm(y~x5)
summary(model5) # r2 = 0.2938
model6 = lm(y~x6)
summary(model6) # r2 = -0.009405
model7 = lm(y~x7)
summary(model7) # r2 = 0.09244
# combined model
combinedModel <- lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(combinedModel)
# the t-values that are most significant are x2 (-1.528), x3 (5.863),
# x5 (6.819), x7 (5.811)
res <- resid(combinedModel)
hist(res)
# plot the residuals vs each predictor variable
plot(x1, res)
plot(x2, res)
plot(x3, res)
plot(x4, res)
plot(x5, res)
plot(x6, res)
plot(x7, res)
# do some sort of transformation and check the model summary on that
y <- log(y)
testModel1 <- lm(y~ x3^2 + x5 + x7)
summary(testModel1)
# Problem 2, Generic Problem set with Regression
generic <- read.csv("generic.csv", sep = ",", header = TRUE)
# getting tired of typing generic$(blah), substituting
y <- generic$y
x1 <- generic$x1
x2 <- generic$x2
x3 <- generic$x3
x4 <- generic$x4
x5 <- generic$x5
x6 <- generic$x6
x7 <- generic$x7
# exploratory data analysis
par(mfrow=c(2,4), mai=c(0.3,0.1,0.2,0.1))
hist(generic$y, main = "Response Histogram") # right-skewed
hist(generic$x1, main = "x1 histogram") # about normal
hist(generic$x2, main = "x2 histogram") # about normal
hist(generic$x3, main = "x3 histogram") # 'uniform' but not
hist(generic$x4, main = "x4 histogram") # about normal
hist(generic$x5, main = "x5 histogram") # 'uniform'
hist(generic$x6, main = "x6 histogram") # almost bimodal?
hist(generic$x7, main = "x7 histogram") # straight up 0 or 1, maybe categorical
# Check some correlations
cor(x1, y) # very low, 0.03185
cor(x2, y) # negative, -0.4132
cor(x3, y) # positive, 0.41152
cor(x4, y) # small negative, -0.00868
cor(x5, y) # positive, big 0.5486
cor(x6, y) # low positive, 0.02811
cor(x7, y) # positive, 0.31876
# do some models and grab summaries from it
model1 = lm(y~x1)
summary(model1) # r^2 horrible
model2 = lm(y~x2)
summary(model2) # r^2 = .1623, ehh
model3 = lm(y~x3)
summary(model3) # r^2 = .1609
model4 = lm(y~x4)
summary(model4) # r2 = -0.01013, bad
model5 = lm(y~x5)
summary(model5) # r2 = 0.2938
model6 = lm(y~x6)
summary(model6) # r2 = -0.009405
model7 = lm(y~x7)
summary(model7) # r2 = 0.09244
# combined model
combinedModel <- lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(combinedModel)
# the t-values that are most significant are x2 (-1.528), x3 (5.863),
# x5 (6.819), x7 (5.811)
res <- resid(combinedModel)
hist(res)
# plot the residuals vs each predictor variable
plot(x1, res)
plot(x2, res)
plot(x3, res)
plot(x4, res)
plot(x5, res)
plot(x6, res)
plot(x7, res)
# do some sort of transformation and check the model summary on that
y <- log(y)
testModel1 <- lm(y~ x3 + x5 + x7^2)
summary(testModel1)
# Problem 2, Generic Problem set with Regression
generic <- read.csv("generic.csv", sep = ",", header = TRUE)
# getting tired of typing generic$(blah), substituting
y <- generic$y
x1 <- generic$x1
x2 <- generic$x2
x3 <- generic$x3
x4 <- generic$x4
x5 <- generic$x5
x6 <- generic$x6
x7 <- generic$x7
# exploratory data analysis
par(mfrow=c(2,4), mai=c(0.3,0.1,0.2,0.1))
hist(generic$y, main = "Response Histogram") # right-skewed
hist(generic$x1, main = "x1 histogram") # about normal
hist(generic$x2, main = "x2 histogram") # about normal
hist(generic$x3, main = "x3 histogram") # 'uniform' but not
hist(generic$x4, main = "x4 histogram") # about normal
hist(generic$x5, main = "x5 histogram") # 'uniform'
hist(generic$x6, main = "x6 histogram") # almost bimodal?
hist(generic$x7, main = "x7 histogram") # straight up 0 or 1, maybe categorical
# Check some correlations
cor(x1, y) # very low, 0.03185
cor(x2, y) # negative, -0.4132
cor(x3, y) # positive, 0.41152
cor(x4, y) # small negative, -0.00868
cor(x5, y) # positive, big 0.5486
cor(x6, y) # low positive, 0.02811
cor(x7, y) # positive, 0.31876
# do some models and grab summaries from it
model1 = lm(y~x1)
summary(model1) # r^2 horrible
model2 = lm(y~x2)
summary(model2) # r^2 = .1623, ehh
model3 = lm(y~x3)
summary(model3) # r^2 = .1609
model4 = lm(y~x4)
summary(model4) # r2 = -0.01013, bad
model5 = lm(y~x5)
summary(model5) # r2 = 0.2938
model6 = lm(y~x6)
summary(model6) # r2 = -0.009405
model7 = lm(y~x7)
summary(model7) # r2 = 0.09244
# combined model
combinedModel <- lm(y~x1+x2+x3+x4+x5+x6+x7)
summary(combinedModel)
# the t-values that are most significant are x2 (-1.528), x3 (5.863),
# x5 (6.819), x7 (5.811)
res <- resid(combinedModel)
hist(res)
# plot the residuals vs each predictor variable
plot(x1, res)
plot(x2, res)
plot(x3, res)
plot(x4, res)
plot(x5, res)
plot(x6, res)
plot(x7, res)
# do some sort of transformation and check the model summary on that
y <- log(y)
testModel1 <- lm(y~ x2^2 + x3 + x5 + x7)
summary(testModel1)
hw <- c(98, 70, 100, 100, 98, 93, 97, 99, 100, 97, 90, 100)
discussions <- c(44.5,43)
project <- 100
midterm <- c(43, 32)
theo_total = 100*length(hw) + 100 + 100 + 100 + 100
cur_total = sum(hw) + sum(discussions) + 100 + sum(midterm)
1500/1600*100
